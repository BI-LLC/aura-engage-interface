#!/usr/bin/env python3
"""
Test script for AURA Voice AI - Document Processing
Phase 3: Test document upload, search, and chat integration
"""

import requests
import json
import time
from pathlib import Path

BASE_URL = "http://localhost:8000"

def create_test_documents():
    """Create test documents for upload"""
    test_dir = Path("test_documents")
    test_dir.mkdir(exist_ok=True)
    
    # Create test TXT file
    txt_file = test_dir / "test_document.txt"
    txt_file.write_text("""
    AURA Voice AI Technical Documentation
    
    AURA is an advanced AI assistant that combines multiple language models
    with voice capabilities and personal knowledge management.
    
    Key Features:
    - Smart routing between Grok and GPT-4 models
    - Voice input using Whisper STT
    - Voice output using ElevenLabs TTS  
    - Personal document knowledge base
    - Session memory for personalized responses
    
    The system uses embeddings to search through uploaded documents
    and provides contextual responses based on your personal content.
    
    Architecture:
    The backend is built with FastAPI and Python, while the frontend
    uses React with TypeScript. Documents are processed into chunks
    and stored with embeddings for semantic search.
    """)
    
    # Create test Markdown file
    md_file = test_dir / "project_notes.md"
    md_file.write_text("""
# Project Notes for AURA

## Overview
AURA is designed to be a personal AI assistant that learns from your documents.

## Technical Stack
- **Backend**: FastAPI, Python 3.12
- **LLMs**: Grok-4, GPT-4-turbo
- **Voice**: Whisper (STT), ElevenLabs (TTS)
- **Storage**: Redis for sessions, filesystem for documents

## Document Processing
Documents are:
1. Uploaded through the API
2. Text extracted based on file type
3. Split into chunks of ~500 tokens
4. Embeddings generated using OpenAI
5. Stored for retrieval during conversations

## Future Enhancements
- Support for more file formats
- Real-time document updates
- Collaborative knowledge bases
- Advanced search filters
    """)
    
    print("‚úÖ Created test documents in 'test_documents' folder")
    return test_dir

def test_document_upload():
    """Test uploading a document"""
    print("\nüìÑ Testing Document Upload...")
    
    test_dir = create_test_documents()
    txt_file = test_dir / "test_document.txt"
    
    with open(txt_file, 'rb') as f:
        files = {'file': ('test_document.txt', f, 'text/plain')}
        response = requests.post(f"{BASE_URL}/documents/upload", files=files)
    
    if response.status_code == 200:
        data = response.json()
        if data['success']:
            print(f"‚úÖ Document uploaded successfully!")
            print(f"   Document ID: {data['document']['id']}")
            print(f"   Chunks created: {data['document']['chunks']}")
            print(f"   Total tokens: {data['document']['tokens']}")
            return data['document']['id']
        else:
            print(f"‚ùå Upload failed: {data}")
    else:
        print(f"‚ùå Request failed: {response.status_code}")
    
    return None

def test_document_list():
    """Test listing documents"""
    print("\nüìö Testing Document List...")
    
    response = requests.get(f"{BASE_URL}/documents/list")
    
    if response.status_code == 200:
        data = response.json()
        print(f"‚úÖ Found {data['count']} documents")
        
        for doc in data['documents']:
            print(f"   - {doc['filename']}")
            print(f"     Chunks: {doc['chunk_count']}, Tokens: {doc['total_tokens']}")
        
        return True
    else:
        print(f"‚ùå Failed to list documents")
        return False

def test_document_search():
    """Test searching documents"""
    print("\nüîç Testing Document Search...")
    
    queries = [
        "What is AURA?",
        "Tell me about the voice features",
        "What technical stack is used?"
    ]
    
    for query in queries:
        print(f"\n   Query: '{query}'")
        
        response = requests.post(
            f"{BASE_URL}/documents/search",
            json={"query": query, "top_k": 2}
        )
        
        if response.status_code == 200:
            data = response.json()
            print(f"   Found {data['count']} relevant chunks")
            
            for i, result in enumerate(data['results'], 1):
                print(f"   Result {i}:")
                print(f"     Document: {result['document']}")
                print(f"     Score: {result['score']:.2%}")
                print(f"     Content: {result['content'][:100]}...")
        else:
            print(f"   ‚ùå Search failed")

def test_chat_with_documents():
    """Test chat with document context"""
    print("\nüí¨ Testing Chat with Document Context...")
    
    questions = [
        "What are the key features of AURA based on the documentation?",
        "How does the document processing work?",
        "What voice technologies are being used?"
    ]
    
    for question in questions:
        print(f"\n   Question: '{question}'")
        
        response = requests.post(
            f"{BASE_URL}/chat",
            json={
                "message": question,
                "use_documents": True,
                "use_memory": False
            }
        )
        
        if response.status_code == 200:
            data = response.json()
            print(f"   Model: {data['model_used']}")
            print(f"   Documents used: {data['documents_used']}")
            print(f"   Response: {data['response'][:200]}...")
            
            if data['documents_used']:
                print("   ‚úÖ Successfully used document context!")
        else:
            print(f"   ‚ùå Chat failed")

def test_knowledge_base_stats():
    """Test knowledge base statistics"""
    print("\nüìä Testing Knowledge Base Stats...")
    
    response = requests.get(f"{BASE_URL}/documents/stats")
    
    if response.status_code == 200:
        data = response.json()
        stats = data['stats']
        
        print("‚úÖ Knowledge Base Statistics:")
        print(f"   Total documents: {stats['total_documents']}")
        print(f"   Total chunks: {stats['total_chunks']}")
        print(f"   Total tokens: {stats['total_tokens']}")
        print(f"   Storage used: {stats['storage_used_mb']} MB")
        print(f"   Estimated cost: ${stats['estimated_cost']:.4f}")
        
        return True
    else:
        print("‚ùå Failed to get stats")
        return False

def test_document_deletion(doc_id):
    """Test deleting a document"""
    print("\nüóëÔ∏è Testing Document Deletion...")
    
    if not doc_id:
        print("   No document ID to delete")
        return False
    
    response = requests.delete(f"{BASE_URL}/documents/{doc_id}")
    
    if response.status_code == 200:
        data = response.json()
        print(f"‚úÖ {data['message']}")
        return True
    else:
        print(f"‚ùå Failed to delete document")
        return False

def main():
    """Run all document processing tests"""
    print("=" * 50)
    print("üöÄ AURA VOICE AI - DOCUMENT PROCESSING TESTS")
    print("üìö Phase 3: Personal Knowledge Base")
    print("=" * 50)
    
    # Check if server is running
    try:
        response = requests.get(BASE_URL)
        data = response.json()
        print(f"‚úÖ Server running: {data['name']} v{data['version']}")
        print(f"   Knowledge base ready: {data['knowledge_base']['ready']}")
        print(f"   Documents loaded: {data['knowledge_base']['documents']}")
    except:
        print("‚ùå Server not running! Start it with:")
        print("   cd backend && python -m app.main")
        return
    
    # Run tests
    tests_passed = 0
    tests_total = 6
    
    # Test 1: Upload document
    doc_id = test_document_upload()
    if doc_id:
        tests_passed += 1
    
    time.sleep(1)  # Give time for processing
    
    # Upload second document
    print("\nüìÑ Uploading second document...")
    test_dir = Path("test_documents")
    md_file = test_dir / "project_notes.md"
    
    with open(md_file, 'rb') as f:
        files = {'file': ('project_notes.md', f, 'text/markdown')}
        response = requests.post(f"{BASE_URL}/documents/upload", files=files)
        if response.status_code == 200:
            print("‚úÖ Second document uploaded")
    
    time.sleep(1)
    
    # Test 2: List documents
    if test_document_list():
        tests_passed += 1
    
    # Test 3: Search documents
    test_document_search()
    tests_passed += 1  # Basic search test
    
    # Test 4: Chat with documents
    print("\n‚è≥ Waiting for embeddings to generate...")
    time.sleep(2)
    test_chat_with_documents()
    tests_passed += 1
    
    # Test 5: Knowledge base stats
    if test_knowledge_base_stats():
        tests_passed += 1
    
    # Test 6: Delete document
    if test_document_deletion(doc_id):
        tests_passed += 1
    
    print("\n" + "=" * 50)
    print(f"‚úÖ TESTS COMPLETE: {tests_passed}/{tests_total} passed")
    print("=" * 50)
    
    print("\nüìã Phase 3 Success Criteria:")
    print("  ‚úÖ Documents uploaded and processed")
    print("  ‚úÖ Text extracted and chunked")
    print("  ‚úÖ Search functionality working")
    print("  ‚úÖ Chat integration with document context")
    print("  ‚úÖ Knowledge base management")
    
    print("\nüéØ Next Steps:")
    print("  1. Upload more documents to build your knowledge base")
    print("  2. Test the web interface at http://localhost:8000/test")
    print("  3. Try asking questions about your uploaded documents")
    print("  4. Monitor embedding costs in the stats endpoint")
    
    print("\nüí° Pro Tips:")
    print("  - Upload documents about topics you frequently discuss")
    print("  - The AI will automatically use relevant document context")
    print("  - Documents are private to each user")
    print("  - Embeddings enable semantic search, not just keyword matching")
    
    if tests_passed == tests_total:
        print("\nüéâ Phase 3 Document Processing: COMPLETE!")
    else:
        print("\n‚ö†Ô∏è Some tests failed - check the logs for details")

if __name__ == "__main__":
    main()